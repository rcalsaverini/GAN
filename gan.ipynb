{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Reshape, Activation, UpSampling2D, Convolution2D, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n",
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(np.min(X_train), np.max(X_train))\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_trainable(net, val):\n",
    "    net.trainable = val\n",
    "    for l in net.layers:\n",
    "        l.trainable = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_40 (InputLayer)            (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_35 (Dense)                 (None, 19600)         1979600     input_40[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_41 (BatchNorm (None, 19600)         39200       dense_35[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_67 (Activation)       (None, 19600)         0           batchnormalization_41[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "reshape_23 (Reshape)             (None, 14, 14, 100)   0           activation_67[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_22 (UpSampling2D)   (None, 28, 28, 100)   0           reshape_23[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_34 (Convolution2D) (None, 28, 28, 50)    45050       upsampling2d_22[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_42 (BatchNorm (None, 28, 28, 50)    100         convolution2d_34[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_68 (Activation)       (None, 28, 28, 50)    0           batchnormalization_42[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_35 (Convolution2D) (None, 28, 28, 25)    11275       activation_68[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_43 (BatchNorm (None, 28, 28, 25)    50          convolution2d_35[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_69 (Activation)       (None, 28, 28, 25)    0           batchnormalization_43[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_36 (Convolution2D) (None, 28, 28, 1)     26          activation_69[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_70 (Activation)       (None, 28, 28, 1)     0           convolution2d_36[0][0]           \n",
      "====================================================================================================\n",
      "Total params: 2075301\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_generator(optimizer, num_channels=100):\n",
    "    generator_input = Input(shape=[100])\n",
    "    hidden_unit = Dense(num_channels*14*14, init='glorot_normal')(generator_input)\n",
    "    hidden_unit = BatchNormalization(mode=2)(hidden_unit)\n",
    "    hidden_unit = Activation('relu')(hidden_unit)\n",
    "    hidden_unit = Reshape( [14, 14, num_channels] )(hidden_unit)\n",
    "    hidden_unit = UpSampling2D(size=(2, 2))(hidden_unit)\n",
    "\n",
    "    hidden_unit = Convolution2D(int(num_channels/2), 3, 3, border_mode='same', init='glorot_uniform')(hidden_unit)\n",
    "    hidden_unit = BatchNormalization(mode=2)(hidden_unit)\n",
    "    hidden_unit = Activation('relu')(hidden_unit)\n",
    "\n",
    "    hidden_unit = Convolution2D(int(nch/4), 3, 3, border_mode='same', init='glorot_uniform')(hidden_unit)\n",
    "    hidden_unit = BatchNormalization(mode=2)(hidden_unit)\n",
    "    hidden_unit = Activation('relu')(hidden_unit)\n",
    "\n",
    "    hidden_unit = Convolution2D(1, 1, 1, border_mode='same', init='glorot_uniform')(hidden_unit)\n",
    "    generator_output = Activation('sigmoid')(hidden_unit)\n",
    "    \n",
    "    generator = Model(generator_input, generator_output)\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return generator\n",
    "\n",
    "optimizer = Adam(lr=1e-4)\n",
    "generator = build_generator(optimizer)\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_41 (InputLayer)            (None, 28, 28, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_37 (Convolution2D) (None, 14, 14, 5)     6405        input_41[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_12 (LeakyReLU)         (None, 14, 14, 5)     0           convolution2d_37[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 14, 14, 5)     0           leakyrelu_12[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_38 (Convolution2D) (None, 7, 7, 512)     64512       dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_13 (LeakyReLU)         (None, 7, 7, 512)     0           convolution2d_38[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 7, 7, 512)     0           leakyrelu_13[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 25088)         0           dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_36 (Dense)                 (None, 256)           6422784     flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_14 (LeakyReLU)         (None, 256)           0           dense_36[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 256)           0           leakyrelu_14[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_37 (Dense)                 (None, 2)             514         dropout_12[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 6494215\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_discriminator(optimizer, shape=(28, 28, 1), dropout_rate=0.1):\n",
    "    discriminator_input = Input(shape=shape)\n",
    "    hidden_unit = Convolution2D(5, 5, 256, subsample=(2, 2), border_mode='same', activation='relu')(discriminator_input)\n",
    "    hidden_unit = LeakyReLU(0.2)(hidden_unit)\n",
    "    hidden_unit = Dropout(dropout_rate)(hidden_unit)\n",
    "    hidden_unit = Convolution2D(512, 5, 5, subsample=(2, 2), border_mode='same', activation='relu')(hidden_unit)\n",
    "    hidden_unit = LeakyReLU(0.2)(hidden_unit)\n",
    "    hidden_unit = Dropout(dropout_rate)(hidden_unit)\n",
    "    hidden_unit = Flatten()(hidden_unit)\n",
    "    hidden_unit = Dense(256)(hidden_unit)\n",
    "    hidden_unit = LeakyReLU(0.2)(hidden_unit)\n",
    "    hidden_unit = Dropout(dropout_rate)(hidden_unit)\n",
    "    discriminator_output = Dense(2,activation='softmax')(hidden_unit)\n",
    "    discriminator = Model(discriminator_input, discriminator_output)\n",
    "    discriminator.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return discriminator\n",
    "\n",
    "d_optimizer = Adam(lr=1e-4)\n",
    "discriminator = build_discriminator(d_optimizer)\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(losses):\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.plot(losses[\"d\"], label='discriminitive loss')\n",
    "        plt.plot(losses[\"g\"], label='generative loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def plot_gen(n_ex=16,dim=(4,4), figsize=(10,10) ):\n",
    "    noise = np.random.uniform(0,1,size=[n_ex,100])\n",
    "    generated_images = generator.predict(noise)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0],dim[1],i+1)\n",
    "        img = generated_images[i,0,:,:]\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_43 (InputLayer)            (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_32 (Model)                 (None, 28, 28, 1)     2075301     input_43[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "model_33 (Model)                 (None, 2)             0           model_32[2][0]                   \n",
      "====================================================================================================\n",
      "Total params: 2075301\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Freeze weights in the discriminator for stacked training\n",
    "make_trainable(discriminator, False)\n",
    "\n",
    "# Build stacked GAN model\n",
    "gan_input = Input(shape=[100])\n",
    "GAN = Model(gan_input, discriminator(generator(gan_input)))\n",
    "GAN.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "GAN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with sess.as_default():\n",
    "    h = GAN(gan_input).eval(feed_dict={gan_input: np.random.normal(size=(10, 100)), K.learning_phase(): True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 5s - loss: 0.0413     \n"
     ]
    }
   ],
   "source": [
    "\n",
    "ntrain = 10000\n",
    "trainidx = random.sample(range(0,X_train.shape[0]), ntrain)\n",
    "XT = X_train[trainidx,:,:,:]\n",
    "\n",
    "# Pre-train the discriminator network ...\n",
    "noise_gen = np.random.uniform(0,1,size=[XT.shape[0],100])\n",
    "generated_images = generator.predict(noise_gen)\n",
    "X = np.concatenate((XT, generated_images))\n",
    "n = XT.shape[0]\n",
    "y = np.zeros([2*n,2])\n",
    "y[:n,1] = 1\n",
    "y[n:,0] = 1\n",
    "\n",
    "make_trainable(discriminator,True)\n",
    "discriminator.fit(X,y, nb_epoch=1, batch_size=128)\n",
    "y_hat = discriminator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-7fb48ec625f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt_frq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Make generative images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "nb_epoch = 5000\n",
    "plt_frq = 25\n",
    "\n",
    "for e in tqdm(range(nb_epoch)):\n",
    "    \n",
    "    # Make generative images\n",
    "    image_batch = X_train[np.random.randint(0,X_train.shape[0],size=BATCH_SIZE),:,:,:]    \n",
    "    noise_gen = np.random.uniform(0,1,size=[BATCH_SIZE,100])\n",
    "    generated_images = generator.predict(noise_gen)\n",
    "    \n",
    "    # Train discriminator on generated images\n",
    "    X = np.concatenate((image_batch, generated_images))\n",
    "    y = np.zeros([2*BATCH_SIZE,2])\n",
    "    y[0:BATCH_SIZE,1] = 1\n",
    "    y[BATCH_SIZE:,0] = 1\n",
    "    \n",
    "    #make_trainable(discriminator,True)\n",
    "    d_loss  = discriminator.train_on_batch(X,y)\n",
    "    losses[\"d\"].append(d_loss)\n",
    "\n",
    "    # train Generator-Discriminator stack on input noise to non-generated output class\n",
    "    noise_tr = np.random.uniform(0,1,size=[BATCH_SIZE,100])\n",
    "    y2 = np.zeros([BATCH_SIZE,2])\n",
    "    y2[:,1] = 1\n",
    "    \n",
    "    #make_trainable(discriminator,False)\n",
    "    g_loss = GAN.train_on_batch(noise_tr, y2 )\n",
    "    losses[\"g\"].append(g_loss)\n",
    "    \n",
    "    # Updates plots\n",
    "    if e%plt_frq==plt_frq-1:\n",
    "        plot_loss(losses)\n",
    "        plot_gen()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
